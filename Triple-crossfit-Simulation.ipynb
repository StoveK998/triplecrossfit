{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import statsmodels.formula.api as smf\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import statsmodels.api as sm\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import  AdaBoostClassifier,AdaBoostRegressor\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\n",
    "from mlxtend.classifier import StackingCVClassifier,StackingClassifier\n",
    "from mlxtend.regressor import StackingCVRegressor,StackingRegressor\n",
    "from sklearn.neural_network import MLPClassifier,MLPRegressor\n",
    "from pygam import GAM,LogisticGAM\n",
    "import itertools\n",
    "from scipy import stats\n",
    "from scipy.special import expit\n",
    "xgb.set_config(verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model setting\n",
    "LG_psmodel = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000)\n",
    "Linear_rspmodel = LinearRegression()\n",
    "\n",
    "RF_psmodel = RandomForestClassifier()\n",
    "RF_rspmodel = RandomForestRegressor()\n",
    "\n",
    "ADA_rspmodel = AdaBoostRegressor(learning_rate=0.05)\n",
    "ADA_psmodel = AdaBoostClassifier()\n",
    "\n",
    "XGB_rspmodel = xgb.XGBRegressor(n_estimators=500,learning_rate=0.005)\n",
    "XGB_psmodel = xgb.XGBClassifier(n_estimators=500,learning_rate=0.005)\n",
    "\n",
    "GAM_rspmodel = GAM(link='identity',n_splines=4,lam=0.6,max_iter=10000)\n",
    "GAM_psmodel =  LogisticGAM(n_splines=4,lam=0.6,max_iter=10000)\n",
    "\n",
    "BPNN_rspmodel = MLPRegressor(hidden_layer_sizes=(128,),activation='relu',max_iter=100,solver='lbfgs')\n",
    "BPNN_psmodel = MLPClassifier(hidden_layer_sizes=(128,),activation='relu',max_iter=100,solver='lbfgs')\n",
    "\n",
    "ps_mds = [LG_psmodel,RF_psmodel,ADA_psmodel,XGB_psmodel,BPNN_psmodel]\n",
    "rsp_mds = [Linear_rspmodel,RF_rspmodel,ADA_rspmodel,GAM_rspmodel]\n",
    "\n",
    "import DRModel\n",
    "dr = DRModel()\n",
    "dr.ml_models(binary_models=ps_mds,continue_models=rsp_mds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other functions\n",
    "def flatten(lis):\n",
    "    ret=[]\n",
    "    for item in lis:\n",
    "        if not isinstance(item,list):\n",
    "            ret.append(item)\n",
    "        else:\n",
    "            ret.extend(flatten(item))\n",
    "    return ret\n",
    "def coverage(p,lower,upper,true_value):\n",
    "    pp=len(p)\n",
    "    count = 0.0\n",
    "    for _ in range(pp):\n",
    "        if true_value[_] >= lower[_] and true_value[_] <= upper[_]:\n",
    "            count+=1.0\n",
    "    return count/pp\n",
    "\n",
    "def I(val_lst,min_thred=-100000,max_thred=100000):\n",
    "    kk = []\n",
    "    for val in val_lst:\n",
    "        if val>=min_thred and val<=max_thred:\n",
    "            kk.append(1)\n",
    "        else:\n",
    "            kk.append(0)\n",
    "    return np.array(kk)\n",
    "\n",
    "def sample_splitting(dataframe,png,ids,resample=False,sample_num=800):\n",
    "#     simu_frame_s1 = dataframe[dataframe['par']==ids] # shuffle\n",
    "    simu_frame_s1 = dataframe\n",
    "    observers_s1 = png[simu_frame_s1.index]\n",
    "    if resample==True:\n",
    "        simu_frame_s = simu_frame_s1.sample(frac=1)\n",
    "        observers_s = png[simu_frame_s.index]\n",
    "    else:\n",
    "        simu_frame_s = simu_frame_s1\n",
    "        observers_s = observers_s1  \n",
    "    true_ate1 = (simu_frame_s[simu_frame_s['Treatment']==1]['Y'].mean() - simu_frame_s[simu_frame_s['Treatment']==0]['Y'].mean())\n",
    "    frame_length = len(simu_frame_s.index)\n",
    "    simu_frame1 = simu_frame_s.iloc[:int(frame_length*0.25),:]\n",
    "    image_observer1 = observers_s[:int(frame_length*0.25),:] #图像切分\n",
    "    \n",
    "    simu_frame2 = simu_frame_s.iloc[int(frame_length*0.25):int(frame_length*0.5),:]\n",
    "    image_observer2 = observers_s[int(frame_length*0.25):int(frame_length*0.5),:] #图像切分\n",
    "    \n",
    "    simu_frame3 = simu_frame_s.iloc[int(frame_length*0.5):int(frame_length*0.75),:]\n",
    "    image_observer3 = observers_s[int(frame_length*0.5):int(frame_length*0.75),:] #图像切分\n",
    "    \n",
    "    simu_frame4 = simu_frame_s.iloc[int(frame_length*0.75):,:]\n",
    "    image_observer4 = observers_s[int(frame_length*0.75):,:] #图像切分\n",
    "    \n",
    "    return true_ate1,simu_frame1,image_observer1,simu_frame2,image_observer2,simu_frame3,image_observer3,simu_frame4,image_observer4   \n",
    "     \n",
    "def dc_sample_splitting(simu_frame_s): \n",
    "    frame_length = len(simu_frame_s.index)\n",
    "    simu_frame1 = simu_frame_s.iloc[:int(frame_length*0.333333),:]\n",
    "    simu_frame2 = simu_frame_s.iloc[int(frame_length*0.333333):int(frame_length*0.666666),:]\n",
    "    simu_frame3 = simu_frame_s.iloc[int(frame_length*0.666666):,:]\n",
    "    \n",
    "    return simu_frame1,simu_frame2,simu_frame3  \n",
    "    \n",
    "def frame_reindex(frame):\n",
    "    a = pd.DataFrame(frame,columns = ['egs'])\n",
    "    a['idx'] = ['es'+str(i) for i in range(len(a))]\n",
    "    a = a.set_index('idx')\n",
    "    return pd.Series(a['egs'])\n",
    "\n",
    "def getate(some_ate,tate,par):\n",
    "    b_ate_hat = {}\n",
    "#     some_ate = baseline_ate\n",
    "    b_indicators = list(some_ate.keys())\n",
    "    sec_keys = ['True','Main Effect','ML']\n",
    "    for indi in some_ate:\n",
    "        b_ate_hat[indi] = {}\n",
    "        for sec_key in sec_keys:\n",
    "            b_ate_hat[indi][sec_key] = []\n",
    "            ate_p = []\n",
    "            sd_p = []\n",
    "            upper_ci_p = []\n",
    "            lower_ci_p = []\n",
    "            for i in partition:\n",
    "                sps_p = []\n",
    "                sps_var = []\n",
    "                par_ate = np.mean(some_ate[indi][sec_key][i],axis=0)\n",
    "                for sps in range(par):\n",
    "                    sp_p = some_ate[indi][sec_key][i][sps]\n",
    "                    sps_p.append(np.mean(sp_p))\n",
    "                    sps_var.append(np.var(sp_p - par_ate[:len(sp_p)], ddof=1)/len(sp_p))\n",
    "                single_ate = np.mean(sps_p)\n",
    "                single_sd = np.sqrt(np.median(sps_var + (sps_p - single_ate)**2))\n",
    "                ate_p.append(single_ate)\n",
    "                sd_p.append(single_sd)\n",
    "                upper_ci_p.append(single_ate+1.96*single_sd)\n",
    "                lower_ci_p.append(single_ate-1.96*single_sd)\n",
    "            for aa in range(len(ate_p)):\n",
    "                b_ate_hat[indi][sec_key].append((ate_p[aa],tate[aa]))\n",
    "    return b_ate_hat\n",
    "\n",
    "def evaluate(some_ate,tate,par):\n",
    "    b_indicators = list(some_ate.keys())\n",
    "    sec_keys = ['True','Main Effect','ML']\n",
    "    for indi in some_ate:\n",
    "        for sec_key in sec_keys:\n",
    "            ate_p = []\n",
    "            sd_p = []\n",
    "            upper_ci_p = []\n",
    "            lower_ci_p = []\n",
    "            for i in partition:\n",
    "                sps_p = []\n",
    "                sps_var = []\n",
    "                par_ate = np.mean(some_ate[indi][sec_key][i],axis=0)\n",
    "                for sps in range(par):\n",
    "                    sp_p = some_ate[indi][sec_key][i][sps]\n",
    "                    sps_p.append(np.mean(sp_p))\n",
    "                    sps_var.append(np.var(sp_p - par_ate[:len(sp_p)], ddof=1)/len(sp_p))\n",
    "                single_ate = np.mean(sps_p)\n",
    "                single_sd = np.sqrt(np.median(sps_var + (sps_p - single_ate)**2))\n",
    "                ate_p.append(single_ate)\n",
    "                sd_p.append(single_sd)\n",
    "                upper_ci_p.append(single_ate+1.96*single_sd)\n",
    "                lower_ci_p.append(single_ate-1.96*single_sd)\n",
    "\n",
    "            ate_hat = np.median(ate_p)\n",
    "            sd = np.median(sd_p)\n",
    "            upper_limit = np.mean(upper_ci_p)\n",
    "            lower_limit = np.mean(lower_ci_p)\n",
    "            CLD = upper_limit-lower_limit\n",
    "            s_df = pd.DataFrame([(ate_hat,sd,'('+str(lower_limit)+','+str(upper_limit)+')',CLD)],columns=['ATE','SD(ATE)','95% CL','CLD'])     \n",
    "\n",
    "            BIAS_lst = np.array(ate_p) - tate\n",
    "            t_BIAS = BIAS_lst.mean()#ate_hat - np.array(tate).mean()\n",
    "            ESE = np.std(BIAS_lst, ddof=1)\n",
    "            RMSE = np.sqrt(np.mean(BIAS_lst**2))\n",
    "            ASE = sd\n",
    "            Coverage = coverage(partition,lower_ci_p,upper_ci_p,tate)\n",
    "            e_df = pd.DataFrame([(t_BIAS,ESE,RMSE)],columns=['BIAS','ESE','RMSE'])     \n",
    "\n",
    "            if list(some_ate.keys()).index(indi)+sec_keys.index(sec_key)==0:\n",
    "                b_summary_df = s_df\n",
    "                b_evaluate_df = e_df\n",
    "            else:\n",
    "                b_summary_df = pd.concat([b_summary_df,s_df])\n",
    "                b_evaluate_df = pd.concat([b_evaluate_df,e_df])\n",
    "    b_summary_df['stats'] = flatten([[i]*len(sec_keys) for i in b_indicators])\n",
    "    b_summary_df['sec_stats'] = sec_keys*len(b_indicators)\n",
    "    b_evaluate_df['stats'] = flatten([[i]*len(sec_keys) for i in b_indicators])\n",
    "    b_evaluate_df['sec_stats'] = sec_keys*len(b_indicators)\n",
    "    b_p_summary_df = b_summary_df.groupby(['stats','sec_stats']).sum()\n",
    "    b_p_evaluate_df = b_evaluate_df.groupby(['stats','sec_stats']).sum()\n",
    "    return b_p_evaluate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import HDFPCA_model\n",
    "HDFPCA = HDFPCA_model()\n",
    "def sample_gene(SampleNumber):\n",
    "    sigma1 = np.random.normal(0, 1, SampleNumber)\n",
    "    sigma2 = np.random.normal(0, 1, SampleNumber)\n",
    "    sigma3 = np.random.normal(0, 1, SampleNumber)\n",
    "    \n",
    "    '''generate eigenimage'''\n",
    "    true_eigenscore,observers = HDFPCA.gen_observers(N=SampleNumber)\n",
    "    coef_eigenscore_a = np.random.uniform(-1,1,50)\n",
    "    coef_eigenscore_b = np.random.uniform(-1,1,100)\n",
    "    eigen_score_a = np.dot(true_eigenscore[:,:50]*true_eigenscore[:,50:],coef_eigenscore_a)\n",
    "    eigen_score_b = np.dot(true_eigenscore**2,coef_eigenscore_b)\n",
    "\n",
    "    Z1 = np.random.binomial(1, 0.5, SampleNumber)\n",
    "    Z2 = np.random.binomial(1, 0.7, SampleNumber)\n",
    "    Z3 = np.random.binomial(1, 0.5, SampleNumber)\n",
    "    Z4 = np.random.binomial(1, 0.3, SampleNumber)\n",
    "    a = np.array([random.choice([12,14,16,16,18,20]) for _ in range(int(0.8*SampleNumber))])\n",
    "    b = np.array([random.choice([13,15,17,19]) for _ in range(int(0.15*SampleNumber))])\n",
    "    c = np.random.randint(4,12,SampleNumber-int(0.95*SampleNumber))\n",
    "    Z5 = np.append(np.append(a,b),c)\n",
    "    random.shuffle(Z5)\n",
    "\n",
    "    d = np.random.normal(72.5, 2, int(0.5*SampleNumber))\n",
    "    e = np.random.normal(77.5, 3, SampleNumber-int(0.5*SampleNumber))\n",
    "    Z6 = np.append(d,e)\n",
    "    random.shuffle(Z6)\n",
    "    X = np.random.binomial(1,expit(-2.2+0.5*eigen_score_a-(Z1+true_eigenscore[:,10])**3/5+(Z2+true_eigenscore[:,30])*(Z3+2)/5+Z5**2/225+((Z4+1)*Z6)/80))\n",
    "    Y = -1+4*X+0.5*eigen_score_b+(((Z1-2*true_eigenscore[:,50])**3))+(Z2+2*true_eigenscore[:,70])**2+3*(Z4+true_eigenscore[:,90])*(Z3-3)**2+Z5**2/100+(np.log(Z6-Z5-30)) + sigma2\n",
    "\n",
    "    simu_frame = pd.DataFrame(Y.reshape(SampleNumber,1),columns=['Y'])\n",
    "    simu_frame['Treatment'] = X\n",
    "    simu_frame['Z1'] = Z1\n",
    "    simu_frame['Z2'] = Z2\n",
    "    simu_frame['Z3'] = Z3\n",
    "    simu_frame['Z4'] = Z4\n",
    "    simu_frame['Z5'] = Z5\n",
    "    simu_frame['Z6'] = Z6\n",
    "    simu_frame['eigen_score_a'] = eigen_score_a\n",
    "    simu_frame['eigen_score_b'] = eigen_score_b\n",
    "    \n",
    "    simu_frame['X_01'] = (Z1+0.5*true_eigenscore[:,10])**3\n",
    "    simu_frame['X_02'] = (Z2+0.5*true_eigenscore[:,30])*(Z3+2)\n",
    "    simu_frame['X_03'] = Z5**2/225\n",
    "    simu_frame['X_04'] = ((Z4+1)*Z6)/80\n",
    "\n",
    "    simu_frame['Y_01'] = (((Z1-2*true_eigenscore[:,50])**3)/3)#(((Z1-2*true_eigenscore[:,50])**3))\n",
    "    simu_frame['Y_02'] = (Z2+true_eigenscore[:,70])**2#(Z2+2*true_eigenscore[:,70])**2\n",
    "    simu_frame['Y_03'] = 3*(Z4)*(Z3-3)**2\n",
    "    simu_frame['Y_04'] = (Z5-2*true_eigenscore[:,70])**2/100\n",
    "    simu_frame['Y_05'] = (np.log(Z6-Z5-30*true_eigenscore[:,90]))  \n",
    "\n",
    "    return simu_frame, observers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = range(100) # set P=100\n",
    "sample_size = 1500 #3000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline \n",
    "baseline_ate = {}\n",
    "baseline_time_lst = {}\n",
    "baseline_ate['G_Computation'] = {}\n",
    "baseline_ate['IPW'] = {}\n",
    "baseline_ate['DR'] = {}\n",
    "baseline_time_lst['G_Computation'] = {}\n",
    "baseline_time_lst['IPW'] = {}\n",
    "baseline_time_lst['DR'] = {}\n",
    "ev_keys = ['True','Main Effect','ML']\n",
    "# caculate computing time\n",
    "part1_time = 0\n",
    "ps_tm = {}\n",
    "rsp_tm = {}\n",
    "# create lists to store estimated ATE\n",
    "for k in ev_keys:\n",
    "    baseline_ate['G_Computation'][k] = {}\n",
    "    baseline_ate['IPW'][k] = {}\n",
    "    baseline_ate['DR'][k] = {}\n",
    "for __ in partition:\n",
    "    sys.stdout.write('\\r'+str(__+1)+'/100')\n",
    "    for k in ev_keys:\n",
    "        baseline_ate['G_Computation'][k][__] = []\n",
    "        baseline_ate['IPW'][k][__] = []\n",
    "        baseline_ate['DR'][k][__] = []   \n",
    "        ps_tm[k] = 0\n",
    "        rsp_tm[k] = 0\n",
    "    simu_frame, observers = sample_gene(sample_size) #generate data\n",
    "    start = time.perf_counter()\n",
    "    eigenscores = HDFPCA.get_est_U(X=observers,N=len(observers),return_eigenscore=True)\n",
    "    eigenframe = pd.DataFrame(eigenscores.T,index=simu_frame.index) \n",
    "    CPV_frame = ((eigenframe.var())/(eigenframe.var()).sum()).cumsum() # caculate CPV\n",
    "    num_of_f = CPV_frame[CPV_frame>0.95].index[0]\n",
    "    # create a dataframe with eigenscore\n",
    "    eigenframe.columns = ['es'+str(i) for i in eigenframe.columns]\n",
    "    simu_frame_a = simu_frame.merge(eigenframe[['es'+str(i) for i in range(num_of_f)]],left_index=True,right_index=True)\n",
    "    # create a formular contains eigenscores\n",
    "    eigen_formu = '+'.join([i for i in simu_frame_a.columns if 'es' in i])\n",
    "    t_simu_frame = simu_frame_a # create a copy of simu_frame\n",
    "    # use all data to caculate dr estimator.\n",
    "    t_simu_frame['par1'] = flatten([i]*int(len(t_simu_frame)/2) for i in range(2)) \n",
    "    part1_time+= time.perf_counter()-start\n",
    "    for cv in range(2):\n",
    "        p2_time = time.perf_counter()\n",
    "        train_simu_frame = t_simu_frame[t_simu_frame['par1']==cv]   \n",
    "        test_simu_frame = t_simu_frame[t_simu_frame['par1']!=cv]  \n",
    "        train_simu_frame['idx'] = range(len(train_simu_frame.index))\n",
    "        train_simu_frame.set_index('idx',inplace=True)\n",
    "        test_simu_frame['idx'] = range(len(test_simu_frame.index))\n",
    "        test_simu_frame.set_index('idx',inplace=True)\n",
    "        dr.data_loadin(train_simu_frame,test_simu_frame) # loadin data\n",
    "        part1_time += time.perf_counter()-p2_time\n",
    "        for sec_keys in ev_keys:\n",
    "            if sec_keys=='True': # caculate propensity score and outcome result based on the true formula\n",
    "                ps_formula = 'Treatment ~eigen_score_a+ X_01 + X_02 + X_03+ X_04'\n",
    "                rsp_formula = 'Y~eigen_score_b+Treatment+ Y_01+Y_02+Y_03+Y_04+Y_05'\n",
    "            else:\n",
    "                ps_formula = 'Treatment ~ Z1 + Z2 + Z3 + Z4 + Z5 + Z6'\n",
    "                rsp_formula = 'Y ~ Treatment + Z1 + Z2 + Z3 + Z4 + Z5 + Z6'\n",
    "\n",
    "                ps_formula = ps_formula+'+'+eigen_formu\n",
    "                rsp_formula = rsp_formula+'+'+eigen_formu\n",
    "            if sec_keys=='ML': # use machine learning method to caculate propensity score and outcome result.\n",
    "                ml_flag=True\n",
    "                test_simu_frame['propensity_score'],ps_time = dr.ps_Model(ps_formula,ml_method=True) \n",
    "                ps_tm[sec_keys]+=ps_time\n",
    "                test_simu_frame['u0_X'],test_simu_frame['u1_X'],rsp_t = dr.rsp_Model(rsp_formula,ml_method=True,model_type='continue')          \n",
    "                rsp_tm[sec_keys]+=rsp_t\n",
    "            else:\n",
    "                ml_flag=False # use Logistic and Linear regression model to caculate propensity score and outcome result.\n",
    "                test_simu_frame['propensity_score'],ps_time = dr.ps_Model(ps_formula,ml_method=False) \n",
    "                ps_tm[sec_keys]+=ps_time\n",
    "                test_simu_frame['u0_X'],test_simu_frame['u1_X'],rsp_t = dr.rsp_Model(rsp_formula,ml_method=False,model_type='continue')     \n",
    "                rsp_tm[sec_keys]+=rsp_t\n",
    "            baseline_data = test_simu_frame[['Treatment','propensity_score','u1_X','u0_X','Y']] \n",
    "            baseline_data['Y_F'] = baseline_data['Y']\n",
    "            #caculate the dr estimator\n",
    "            baseline_ate_pos = (baseline_data['Treatment']*(baseline_data['Y_F']-baseline_data['u1_X']))/baseline_data['propensity_score'] + baseline_data['u1_X']\n",
    "            baseline_ate_neg = ((1.0-baseline_data['Treatment'])*(baseline_data['Y_F']-baseline_data['u0_X']))/(1.0-baseline_data['propensity_score']) + baseline_data['u0_X']           \n",
    "            # caculate the DR/IPW/G_Computation estimator\n",
    "            DR = baseline_ate_pos-baseline_ate_neg\n",
    "            G_Computation = baseline_data['u1_X'] - baseline_data['u0_X']\n",
    "            IPW = ((baseline_data['Treatment']*baseline_data['Y_F'])/baseline_data['propensity_score'])-(((1.0-baseline_data['Treatment'])*baseline_data['Y_F'])/(1.0-baseline_data['propensity_score']))\n",
    "\n",
    "            baseline_ate['G_Computation'][sec_keys][__].append(G_Computation)\n",
    "            baseline_ate['IPW'][sec_keys][__].append(IPW)\n",
    "            baseline_ate['DR'][sec_keys][__].append(DR)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline computing time\n",
    "print (('IPW-ML:')+str((part1_time+ps_tm['ML'])))\n",
    "print (('IPW-MAIN:')+str((part1_time+ps_tm['Main Effect'])))\n",
    "print (('IPW-TRUE:')+str((part1_time+ps_tm['True'])))\n",
    "print (('G-ML:')+str((part1_time+rsp_tm['ML'])))\n",
    "print (('G-MAIN:')+str((part1_time+rsp_tm['Main Effect'])))\n",
    "print (('G-TRUE:')+str((part1_time+rsp_tm['True'])))\n",
    "print (('DR-ML:')+str((part1_time+rsp_tm['ML']+ps_tm['ML'])))\n",
    "print (('DR-MAIN:')+str((part1_time+rsp_tm['Main Effect']+ps_tm['Main Effect'])))\n",
    "print (('DR-TRUE:')+str((part1_time+rsp_tm['True']+ps_tm['True'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline result\n",
    "e_df = evaluate(baseline_ate,np.array([4]*len(partition)), 2)\n",
    "e_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_ate = {}\n",
    "dc_time_lst = {}\n",
    "dc_ate['DC_DR'] = {}\n",
    "dc_time_lst['DC_DR'] = {}\n",
    "splits = [1,2,3]\n",
    "dc_true_ate = []\n",
    "for k in ev_keys:\n",
    "    dc_ate['DC_DR'][k] = {}\n",
    "    dc_time_lst['DC_DR'][k] = []\n",
    "for __ in partition:\n",
    "    simu_frame, observers = sample_gene(sample_size) #create data\n",
    "    start = time.perf_counter()\n",
    "    eigenscores = HDFPCA.get_est_U(X=observers,N=len(observers),return_eigenscore=True)\n",
    "    eigenframe = pd.DataFrame(eigenscores.T,index=simu_frame.index) \n",
    "    CPV_frame = ((eigenframe.var())/(eigenframe.var()).sum()).cumsum()\n",
    "    num_of_f = CPV_frame[CPV_frame>0.95].index[0]\n",
    "    eigenframe.columns = ['es'+str(i) for i in eigenframe.columns]\n",
    "    simu_frame_a = simu_frame.merge(eigenframe[['es'+str(i) for i in range(num_of_f)]],left_index=True,right_index=True)\n",
    "    eigen_formu = '+'.join([i for i in simu_frame_a.columns if 'es' in i])\n",
    "    t_simu_frame = simu_frame_a\n",
    "\n",
    "    sys.stdout.write('\\r'+str(__+1)+'/100')\n",
    "    # cross-modeling\n",
    "    simu_frame1,simu_frame2,simu_frame3 = dc_sample_splitting(t_simu_frame)\n",
    "\n",
    "    # hdfpca / psm-model / response\n",
    "    for o in splits:\n",
    "        eval('simu_frame'+str(o))['idx'] = range(len(eval('simu_frame'+str(o)).index))\n",
    "        eval('simu_frame'+str(o)).set_index('idx',inplace=True)\n",
    "    for kys in ev_keys:\n",
    "        dc_ate['DC_DR'][kys][__] = []\n",
    "    combi_lst = (list(itertools.permutations(splits, 3)))\n",
    "    part1_time+=time.perf_counter()-start # caculate the computing time of the above code\n",
    "    '''\n",
    "    In the code below, we cut the data into 3 splits; \n",
    "        the first split is used to caculate propensity score model,\n",
    "        the second split is used to caculate outcome model,\n",
    "        the third split is used to caculate DR estimator.\n",
    "    In \"combi_lst\", all the different combinations of numbers will occur in sequence, \n",
    "        and we only need to determine the position of the numbers in the list of combinations,\n",
    "        that is how the function \"eval\" works.\n",
    "    '''\n",
    "    for combi in combi_lst:       \n",
    "        simu_frame11 = simu_frame1.copy()\n",
    "        simu_frame12 = simu_frame2.copy()\n",
    "        simu_frame13 = simu_frame3.copy()\n",
    "        # propensity score        \n",
    "        for sec_keys in ev_keys:\n",
    "            if sec_keys=='True':\n",
    "                ps_formula = 'Treatment ~eigen_score_a+ X_01 + X_02 + X_03+ X_04'\n",
    "                rsp_formula = 'Y~eigen_score_b+Treatment+ Y_01+Y_02+Y_03+Y_04+Y_05'\n",
    "            else:\n",
    "                ps_formula = 'Treatment ~ Z1 + Z2 + Z3 + Z4 + Z5 + Z6'\n",
    "                rsp_formula = 'Y ~ Treatment + Z1 + Z2 + Z3 + Z4 + Z5 + Z6'\n",
    "\n",
    "                ps_formula = ps_formula+'+'+eigen_formu\n",
    "                rsp_formula = rsp_formula+'+'+eigen_formu\n",
    "            if sec_keys=='ML':\n",
    "                ml_flag=True\n",
    "                dr.data_loadin(eval('simu_frame1'+str(combi[1])),eval('simu_frame1'+str(combi[0])))\n",
    "                eval('simu_frame1'+str(combi[0]))['propensity_score'],ps_time = dr.ps_Model(ps_formula,ml_flag) \n",
    "                ps_tm[sec_keys]+=ps_time\n",
    "                dr.data_loadin(eval('simu_frame1'+str(combi[2])),eval('simu_frame1'+str(combi[0])))\n",
    "                eval('simu_frame1'+str(combi[0]))['u0_X'],eval('simu_frame1'+str(combi[0]))['u1_X'],rsp_t = dr.rsp_Model(rsp_formula,ml_method=True,model_type='continue')          \n",
    "                rsp_tm[sec_keys]+=rsp_t\n",
    "            else:\n",
    "                ml_flag=False\n",
    "                dr.data_loadin(eval('simu_frame1'+str(combi[1])),eval('simu_frame1'+str(combi[0])))\n",
    "                eval('simu_frame1'+str(combi[0]))['propensity_score'],ps_time = dr.ps_Model(ps_formula,ml_method=False) \n",
    "                ps_tm[sec_keys]+=ps_time\n",
    "                dr.data_loadin(eval('simu_frame1'+str(combi[2])),eval('simu_frame1'+str(combi[0])))\n",
    "                eval('simu_frame1'+str(combi[0]))['u0_X'],eval('simu_frame1'+str(combi[0]))['u1_X'],rsp_t = dr.rsp_Model(rsp_formula,ml_method=False,model_type='continue')     \n",
    "                rsp_tm[sec_keys]+=rsp_t\n",
    "\n",
    "            data = eval('simu_frame1'+str(combi[0]))[['Treatment','propensity_score','u1_X','u0_X','Y']] \n",
    "            data['Y_F'] = data['Y']\n",
    "            ate_pos = (data['Treatment']*(data['Y_F']-data['u1_X']))/data['propensity_score'] + data['u1_X']\n",
    "            ate_neg = ((1.0-data['Treatment'])*(data['Y_F']-data['u0_X']))/(1.0-data['propensity_score']) + data['u0_X']           \n",
    "            DC_DR = ate_pos-ate_neg\n",
    "\n",
    "            dc_ate['DC_DR'][sec_keys][__].append(DC_DR)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the run-time of DR-DC \n",
    "print ((ps_tm['ML']+rsp_tm['ML']+part1_time))\n",
    "print ((ps_tm['Main Effect']+rsp_tm['Main Effect']+part1_time))\n",
    "print ((ps_tm['True']+rsp_tm['True']+part1_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the result of DR-DC\n",
    "e_df = evaluate(dc_ate,np.array([4]*100), 6) # 6 indicates the number of different combinations in DC-DR\n",
    "e_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# main simulation section\n",
    "splits = [1,2,3,4]\n",
    "part1_time = 0\n",
    "ps_tm = {}\n",
    "rsp_tm = {}\n",
    "partition_ate = {}\n",
    "partition_time_lst = {}\n",
    "partition_ate['TC_DR'] = {}\n",
    "partition_time_lst['TC_DR'] = {}\n",
    "for k in ev_keys:\n",
    "    partition_ate['TC_DR'][k] = {}\n",
    "    partition_time_lst['TC_DR'][k] = []\n",
    "for __ in partition:\n",
    "    sys.stdout.write('\\r'+str(__+1)+'/100')\n",
    "    # cross-modeling\n",
    "    simu_frame, observers = sample_gene(sample_size)\n",
    "    true,simu_frame1,image_observer1,simu_frame2,image_observer2,simu_frame3,image_observer3,simu_frame4,image_observer4 = sample_splitting(simu_frame,observers,0,resample=False,sample_num=sample_n) \n",
    "    # hdfpca / psm-model / response    \n",
    "    for o in splits:\n",
    "        eval('simu_frame'+str(o))['idx'] = range(len(simu_frame1.index))\n",
    "        eval('simu_frame'+str(o)).set_index('idx',inplace=True)\n",
    "    for kys in ev_keys:\n",
    "        partition_ate['TC_DR'][kys][__] = []\n",
    "        ps_tm[kys] = 0\n",
    "        rsp_tm[kys] = 0\n",
    "    for _ in splits:   \n",
    "        start = time.perf_counter()\n",
    "        est_u = HDFPCA.get_est_U(X=eval('image_observer'+str(_)),N=len(image_observer1))\n",
    "        est_u_inv = est_u.T \n",
    "        sec_splits = splits.copy()\n",
    "        sec_splits.remove(_)\n",
    "        combi_lst = (list(itertools.permutations(sec_splits, 3)))      \n",
    "        '''\n",
    "        In the following code, we caculate the eigenscore list (CPV>95%) with each split,\n",
    "            and select the eigenscore uniformly before entering the combinatorial loop.\n",
    "            This work is to prevent slight differences in the number of eigenscores when computed with different splits.\n",
    "        (note: in simulation study, the probability of this happening is small, \n",
    "                but for consistency with the code of real data study, it is still used here)\n",
    "        '''\n",
    "        eigenframe1 = pd.DataFrame(np.matmul(est_u_inv,get_X_bar(X=image_observer1,N=len(image_observer1)).T).T)    \n",
    "        CPV_frame = ((eigenframe1.var())/((eigenframe1.var()).sum())).sort_values(ascending = False).cumsum()\n",
    "        eigenframe1 = eigenframe1.reindex(CPV_frame.index)\n",
    "        CPV_frame1 = frame_reindex(CPV_frame)    \n",
    "        eigenframe1.columns = CPV_frame1.index\n",
    "        \n",
    "        eigenframe2 = pd.DataFrame(np.matmul(est_u_inv,get_X_bar(X=image_observer2,N=len(image_observer2)).T).T)  \n",
    "        CPV_frame = ((eigenframe2.var())/((eigenframe2.var()).sum())).sort_values(ascending = False).cumsum()\n",
    "        eigenframe2 = eigenframe2.reindex(CPV_frame.index)\n",
    "        CPV_frame2 = frame_reindex(CPV_frame) \n",
    "        eigenframe2.columns = CPV_frame2.index\n",
    "\n",
    "        eigenframe3 = pd.DataFrame(np.matmul(est_u_inv,get_X_bar(X=image_observer3,N=len(image_observer3)).T).T)    \n",
    "        CPV_frame = ((eigenframe3.var())/((eigenframe3.var()).sum())).sort_values(ascending = False).cumsum()\n",
    "        eigenframe3 = eigenframe3.reindex(CPV_frame.index)\n",
    "        CPV_frame3 = frame_reindex(CPV_frame) \n",
    "        eigenframe3.columns = CPV_frame3.index\n",
    "        \n",
    "        eigenframe4 = pd.DataFrame(np.matmul(est_u_inv,get_X_bar(X=image_observer4,N=len(image_observer4)).T).T) \n",
    "        CPV_frame = ((eigenframe4.var())/((eigenframe4.var()).sum())).sort_values(ascending = False).cumsum()\n",
    "        eigenframe4 = eigenframe4.reindex(CPV_frame.index)\n",
    "        CPV_frame4 = frame_reindex(CPV_frame) \n",
    "        eigenframe4.columns = CPV_frame4.index\n",
    "        \n",
    "        \n",
    "        choose_index = eval('CPV_frame'+str(_))[eval('CPV_frame'+str(_))<0.95].index\n",
    "        simu_frame1[[i for i in choose_index]] = eigenframe1[[i for i in choose_index]]\n",
    "        simu_frame2[[i for i in choose_index]] = eigenframe2[[i for i in choose_index]]\n",
    "        simu_frame3[[i for i in choose_index]] = eigenframe3[[i for i in choose_index]]\n",
    "        simu_frame4[[i for i in choose_index]] = eigenframe4[[i for i in choose_index]]\n",
    "        part1_time+=time.perf_counter()-start\n",
    "        for combi in combi_lst: #【2,3,4】\n",
    "            simu_frame11 = simu_frame1.copy()\n",
    "            simu_frame12 = simu_frame2.copy()\n",
    "            simu_frame13 = simu_frame3.copy()\n",
    "            simu_frame14 = simu_frame4.copy()\n",
    "            \n",
    "            eigenframe1 = pd.DataFrame(np.matmul(est_u_inv,HDFPCA.get_X_bar(X=eval('image_observer'+str(combi[0])),N=len(eval('image_observer'+str(combi[0])))).T).T)  \n",
    "            CPV_frame = ((eigenframe1.var())/(eigenframe1.var()).sum()).cumsum()\n",
    "            num_of_f = CPV_frame[CPV_frame>0.95].index[0]\n",
    "            eigenframe1.columns = ['es'+str(i) for i in eigenframe1.columns]\n",
    "            eval('simu_frame1'+str(combi[0]))[eigenframe1.columns[:num_of_f]] = eigenframe1[['es'+str(i) for i in range(num_of_f)]]\n",
    "\n",
    "            eigenframe2 = pd.DataFrame(np.matmul(est_u_inv,HDFPCA.get_X_bar(X=eval('image_observer'+str(combi[1])),N=len(eval('image_observer'+str(combi[1])))).T).T)  \n",
    "            eigenframe2.columns = ['es'+str(i) for i in eigenframe2.columns]\n",
    "            eval('simu_frame1'+str(combi[1]))[eigenframe2.columns[:num_of_f]] = eigenframe2[['es'+str(i) for i in range(num_of_f)]]\n",
    "            eigenframe3 = pd.DataFrame(np.matmul(est_u_inv,HDFPCA.get_X_bar(X=eval('image_observer'+str(combi[2])),N=len(eval('image_observer'+str(combi[2])))).T).T)   \n",
    "            eigenframe3.columns = ['es'+str(i) for i in eigenframe3.columns]\n",
    "            eval('simu_frame1'+str(combi[2]))[eigenframe3.columns[:num_of_f]] = eigenframe3[['es'+str(i) for i in range(num_of_f)]]     \n",
    "            for sec_keys in ev_keys:\n",
    "                if sec_keys=='True':\n",
    "                    ps_formula = 'Treatment ~Za_image+ X_01 + X_02 + X_03+ X_04'\n",
    "                    rsp_formula = 'Y~Zb_image+Treatment+ Y_01+Y_02+Y_03+Y_04+Y_05'\n",
    "                else:\n",
    "                    ps_formula = 'Treatment ~ Z1 + Z2 + Z3 + Z4 + Z5 + Z6'\n",
    "                    rsp_formula = 'Y ~ Treatment + Z1 + Z2 + Z3 + Z4 + Z5 + Z6'\n",
    "                ps_formula += '+'+'+'.join([i for i in simu_frame2.columns if 'es' in i])\n",
    "                rsp_formula += '+'+'+'.join([i for i in simu_frame2.columns if 'es' in i])\n",
    "                # propensity score\n",
    "                if sec_keys=='ML':\n",
    "                    ml_flag=True\n",
    "                    dr.data_loadin(eval('simu_frame1'+str(combi[1])),eval('simu_frame1'+str(combi[0])))\n",
    "                    eval('simu_frame1'+str(combi[0]))['propensity_score'],ps_time = dr.ps_Model(ps_formula,ml_flag) \n",
    "                    ps_tm[sec_keys]+=ps_time\n",
    "                    dr.data_loadin(eval('simu_frame1'+str(combi[2])),eval('simu_frame1'+str(combi[0])))\n",
    "                    eval('simu_frame1'+str(combi[0]))['u0_X'],eval('simu_frame1'+str(combi[0]))['u1_X'],rsp_t = dr.rsp_Model(rsp_formula,ml_method=True,model_type='continue')          \n",
    "                    rsp_tm[sec_keys]+=rsp_t\n",
    "                else:\n",
    "                    ml_flag=False\n",
    "                    dr.data_loadin(eval('simu_frame1'+str(combi[1])),eval('simu_frame1'+str(combi[0])))\n",
    "                    eval('simu_frame1'+str(combi[0]))['propensity_score'],ps_time = dr.ps_Model(ps_formula,ml_method=False) \n",
    "                    ps_tm[sec_keys]+=ps_time\n",
    "                    dr.data_loadin(eval('simu_frame1'+str(combi[2])),eval('simu_frame1'+str(combi[0])))\n",
    "                    eval('simu_frame1'+str(combi[0]))['u0_X'],eval('simu_frame1'+str(combi[0]))['u1_X'],rsp_t = dr.rsp_Model(rsp_formula,ml_method=False,model_type='continue')     \n",
    "                    rsp_tm[sec_keys]+=rsp_t\n",
    "                data = eval('simu_frame1'+str(combi[0]))[['Treatment','propensity_score','u1_X','u0_X','Y']] \n",
    "                data['Y_F'] = data['Y']\n",
    "                ate_pos = (data['Treatment']*(data['Y_F']-data['u1_X']))/data['propensity_score'] + data['u1_X']\n",
    "                ate_neg = ((1.0-data['Treatment'])*(data['Y_F']-data['u0_X']))/(1.0-data['propensity_score']) + data['u0_X']           \n",
    "\n",
    "                TC_DR = ate_pos-ate_neg\n",
    "                partition_ate['TC_DR'][sec_keys][__].append(TC_DR)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the run-time of DR-TC \n",
    "print ((ps_tm['ML']+rsp_tm['ML']+part1_time))\n",
    "print ((ps_tm['Main Effect']+rsp_tm['Main Effect']+part1_time))\n",
    "print ((ps_tm['True']+rsp_tm['True']+part1_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_df = evaluate(partition_ate,np.array([4]*len(partition)), 24) # 24 indicates the number of different combinations in DC-DR\n",
    "e_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
